from dotenv import load_dotenv

from ..prompts.system_prompts import SUPERVISOR_SYSTEM_PROMPT, AGGREGATOR_SYSTEM_PROMPT
from .agent_members import *
from .agent_state import RouteResponse

from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_nvidia import ChatNVIDIA
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

load_dotenv()

model = ChatOpenAI(model="gpt-4o")
#model = ChatNVIDIA(model="meta/llama-3.1-405b-instruct")
#model = ChatOllama(model="llama3.2")

def supervisor_node(state):
    """
        Supervisory agent forwards the task to the appropriate agent and reviews 
        whether the user's question has already been answered.

        Args:
            state: State of the graph
        
        Returns:
            Model response
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", SUPERVISOR_SYSTEM_PROMPT),
        MessagesPlaceholder(variable_name="messages"),
        (
            "system",
            "Given the conversation above, analyze the context and decide whether the user's question was answered. If it has not yet been answered, decide which agent should be called next. "
            "If everything is resolved, return 'FINISH'. Select only one of the options: {options}."
            "Before selecting, make sure the task has progressed."
            "If the same task has been repeated or appears redundant, return with 'FINISH'."
        )
    ]).partial(options=str(OPTIONS), members=", ".join(MEMBERS))

    supervisor_chain = prompt | model.with_structured_output(RouteResponse)
    result = supervisor_chain.invoke(state)
    return result

def aggregator_node(state):
    """
        Receives a state with messages generated by other agents, 
        sends this information to the language model and returns a response

        Args:
            state: State of the graph

        Returns:
            Model response
    """
    user_query = next(
        (msg.content for msg in state["messages"] if isinstance(msg, HumanMessage)), 
        "No questions found."
    )
    
    '''agent_responses = next(
        (msg.content for msg in state["messages"] if isinstance(msg, AIMessage)),
        "Nenhuma resposta encontrada."
    )'''
    agent_responses = "\n".join(
        msg.content for msg in state["messages"] if isinstance(msg, AIMessage)
    )
    
    messages = [
        ("system", AGGREGATOR_SYSTEM_PROMPT),
        (
            "assistant",
            f"Please add the following information:\n\n"
            f"User question: {user_query}\n\n"
            f"Answers found:\n{agent_responses}"
        ),
    ]
    #print(state)
    #print("\n\n")
    #print(messages)
    response = model.invoke(messages)
    return {
        "messages": [
            AIMessage(content=response.content, name="Agente_Agregador")
        ]
    }